<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">

        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>FETA</title>
        <meta name="description" content="FETA is a benchmark for Few-Shot Task Transfer in Open-Domain Dialogue.">
        <meta name="keywords" content="Few shot,Task Transfer,Dialogue,Chit Chat, Open Domain Dialogue">
        <meta name="author" content="FETA">

        <link rel="icon" href="image/favicon.ico">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/fonts.css">
        <link rel="stylesheet" href="css/custom.css">
    </head>

        <nav class="navbar navbar-expand-lg navbar-dark navbar-custom fixed-top">
            <div class="container">
                <a class="navbar-brand" href="index.html"><b>FETA</b></a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#content" aria-controls="navbarsExample07" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse" id="content">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="index.html"><b>Home</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://github.com/alon-albalak/TLiDB/blob/master/FETA_README.md">Challenge</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="download.html">Download</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://codalab.lisn.upsaclay.fr/competitions/10745">FETA-DailyDialog Leaderboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://codalab.lisn.upsaclay.fr/competitions/10744">FETA-Friends Leaderboard</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://github.com/alon-albalak/TLiDB/blob/master/FETA_README.md#sample-code">Starter Code</a>
                        <li class="nav-item">
                            <a class="nav-link" href="https://aclanthology.org/2022.emnlp-main.751/" target="_blank">Paper</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="mailto:feta.benchmark@gmail.com">Contact</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <br/>
        <br/>

         <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>How do I get the benchmark data?</h4>
                </div>
            </div>

            <hr>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>The benchmark data can be accessed through the <a href="https://github.com/alon-albalak/TLiDB">Transfer Learning in Dialogue Benchmarking Toolkit (TLiDB link)</a> with just a few simple lines of code.</p>
                    <p>We highly recommend to install and utilize the benchmark through the TLiDB package, but the raw datasets can also be downloaded.</p>
                    
                </div>
                
            </div>

            <br/>

            <div class="container" align="justify">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>To Download Data Through TLiDB [Recommended]:</h4>
                    <hr>
                    <h6>Note: TLiDB does not require an explicit downloading step. By using the TLiDB dataloader, the datasets will automatically be downloaded</h6>
                    <h5>Quickstart (2 options):</h5>
                    <ol>
                    <li><b>To install TLiDB with pip (recommended for recreating baselines):</b> 
                        <br>                        
                        Install TLiDB with pip: <pre><code>pip install tlidb</code></pre>
                        Then models can quickly be trained directly from the command line such as:<pre><code>tlidb --source_datasets Friends --source_tasks emory_emotion_recognition --target_datasets Friends --target_tasks reading_comprehension --do_train --do_finetune --do_eval --eval_best --model_config=bert</code></pre>
                    </li>
                    <li>
                        <b>To utilize the TLiDB dataloader (recommended for use with your own models and training script):</b>
                        <br>
                        Install TLiDB with pip as above: <pre><code>pip install tlidb</code></pre>

                        <h6>OR</h6>Install TLiDB from source:
                        <pre><code>
git clone git@github.com:alon-albalak/TLiDB.git
cd TLiDB
pip install -e .
                        </code></pre>
                        <b>Then</b>, follow <a href="https://github.com/alon-albalak/TLiDB/tree/master/tlidb/TLiDB#data-loading">these instructions</a> on data loading to incorporate TLiDB dataloaders into your own script.
                    </li>
                    </ol>
                    
                </div>
            </div>

            <br/>

            <div class="container" align="justify">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>To Download Raw Data:</h4>
                    <hr>
                    <h6>Note: These links will download a zipped version of each dataset. Each raw dataset contains 6 files: the raw dataset (eg. TLiDB_Friends.json), 3 files that identify train/dev/test dialogue IDs for full-data splits (eg. TTiDB_test_ids.txt), 2 files that identify train/dev dialogue IDs for few-shot splits (eg. TTiDB_0.1_percent_few_shot_train_ids.txt)</h6>
                    <h5>Warning: Raw data does not contain prompts, and will require writing dataloaders that already exist in TLiDB.</h5>
                </div>
            </div>
            <div class="row" align="center">
                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <h5>FETA-DailyDialog: <a href="https://drive.google.com/uc?export=download&id=1c88IN_ZHpRvkkgQxmN4sfGmqN2DurQx2" target="_blank"><img src="image/download.png" style="height:4rem; width:4rem;"/></a></h5> 

                </div>
                <div class="col-lg-4 col-md-6 col-sm-12 col-xs-12 thumb">
                    <h5>FETA-Friends: <a href="https://drive.google.com/uc?export=download&id=1QK_XX-d38fKeJlcTcoMT9ku3VZn6bv6H" target="_blank"><img src="image/download.png" style="height:4rem; width:4rem;"/></a></h5>
                </div>
            </div>

        </div>

        <br/>
        <br/>


        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Dataset Schema</h4>
                </div>
            </div>

            <hr>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <pre><code>
{
    "metadata": {
        "dataset_name": "Dataset Name",
        "tasks": [ # list of task names
            "task1",
            "task2",
        ],
        "task_metadata": { # metadata about tasks, for example: labels, metrics, or metric keyword arguments 
            "task_1": {
                "labels": [
                    "label1",
                    "label2"
                ],
                "metrics": [
                    "f1"
                ]
            },
            "task_2":{
                "labels": [
                    "label1",
                    "label2",
                    "label3"
                ]
            }
        }
    },
    "data": [ # list of dicts
        {
            "dialogue_id": "dialogue-1",
            "dialogue_metadata":{ # can be used to determine which tasks exist in this dialogue
                "dialogue-level-classification-task1": null,
                "dialogue-level-classification-task2": null,
                "turn-level-classification-task1": null,
                "turn-level-classification-task2": null,
            }
            "dialogue-level-classification-task1": {
                "label": "ground truth label",
                "instance_id": instance_id
            ,
            "dialogue-level-classification-task2": {
                "label": "ground truth label",
                "instance_id": instance_id
            },
            "dialogue": [ # list of dicts
                {
                    "turn_id": "1",
                    "speakers": ["speaker1"],
                    "utterance": "Example utterance",
                    "turn-level-classification-task1": {
                        "label": "ground truth label",
                        "instance_id": instance_id
                    },
                    "turn-level-classification-task2": {
                        "label": "ground truth label",
                        "instance_id": instance_id
                    },
                },
                {
                    "turn_id": "2",
                    "speakers": ["speaker2"],
                    "utterance": "Second example utterance",
                    "turn-level-classification-task1": {
                        "label": "ground truth label",
                        "instance_id": instance_id
                    },
                    "turn-level-classification-task2": {
                        "label": "ground truth label",
                        "instance_id": instance_id
                    }
                }
            ]
        }
    ]
}
                </code></pre>
                </div>
            </div>
        </div>

            <!-- <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h6>Note: we will hold the annotations of the test sets for challenge use, but you can submit the results to our <a href="https://competitions.codalab.org/competitions/24360">VATEX Video Captioning Challenge</a> and <a href="https://competitions.codalab.org/competitions/24384">Video-guided Machine Translation Challenge</a> for evaluation. The challenges will be always valid and never expire.
                    The split of the validation set is not mandatory, and you are allowed to submit the testing results by a model trained on both training & validation sets.</h6>
                </div>

                <br/>
                <br/>

                <div class="col-lg-3 col-md-3 col-sm-12 col-xs-12 thumb">
                    <h5>Training Set</h5>
                    <ul>
                        <li>25,991 Videos</li>
                        <li>259,910 English Captions</li>
                        <li>259,910 Chinese Captions</li>
                    </ul>
                    <p><a href="./data/vatex_training_v1.0.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;"/>
                    </a>
                    (v1.0, 57.3 MB)</p>
                </div>

                <div class="col-lg-3 col-md-3 col-sm-12 col-xs-12 thumb">
                    <h5>Validation Set</h5>
                    <ul>
                        <li>3,000 Videos</li>
                        <li>30,000 English Captions</li>
                        <li>30,000 Chinese Captions</li>
                    </ul>
                    <p><a href="./data/vatex_validation_v1.0.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;float: "/>
                    </a>
                    (v1.0, 6.6 MB)</p>
                </div>

                <div class="col-lg-3 col-md-3 col-sm-12 col-xs-12 thumb">
                    <h5>Public Test Set</h5>
                    (English released for VMT evaluation)
                    <ul>
                        <li>6,000 Videos</li>
                        <li>6,0000 English Captions</li>
                    </ul>
                    <p><a href="./data/vatex_public_test_english_v1.1.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;float: "/>
                    </a>
                    (v1.1, 4.9 MB)</p>
                </div>

                <div class="col-lg-3 col-md-3 col-sm-12 col-xs-12 thumb">
                    <h5>Private Test Set</h5>
                    (heldout for caption evaluation)
                    <ul>
                        <li>6,278 Videos</li>
                        <br>
                    </ul>
                    <p><a href="./data/vatex_private_test_without_annotations.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;"/>
                    </a>
                    (v1.1, 0.26 MB)</p>
                </div>
            </div>
        </div>

        <br/>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>VATEX v1.0 (for use of VATEX Captioning Challenge 2019)</h4>
                </div>
            </div>

            <hr>

            <div class="row">

                <br/>

                <div class="col-lg-4 col-md-4 col-sm-12 col-xs-12 thumb">
                    <h5>Training Set</h5>
                    <ul>
                        <li>25,991 Videos</li>
                        <li>259,910 English Captions</li>
                        <li>259,910 Chinese Captions</li>
                    </ul>
                    <p><a href="./data/vatex_training_v1.0.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;"/>
                    </a>
                    (v1.0, 57.3 MB)</p>
                </div>

                <div class="col-lg-4 col-md-4 col-sm-12 col-xs-12 thumb">
                    <h5>Validation Set</h5>
                    <ul>
                        <li>3,000 Videos</li>
                        <li>30,000 English Captions</li>
                        <li>30,000 Chinese Captions</li>
                    </ul>
                    <p><a href="./data/vatex_validation_v1.0.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;float: "/>
                    </a>
                    (v1.0, 6.6 MB)</p>
                </div>

                <div class="col-lg-4 col-md-4 col-sm-12 col-xs-12 thumb">
                    <h5>Public Test Set</h5>
                    <ul>
                        <li>6,000 Videos</li>
                        <br>
                        <br>
                    </ul>
                    <p><a href="./data/vatex_public_test_without_annotations.json" target="_blank">
                        <img src="image/download.png" style="height:4rem; width:4rem;"/>
                    </a>
                    (v1.0, 0.25 MB)</p>
                </div>
            </div>
        </div>

        <br>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Pretrained Video Features</h4>
                </div>
            </div>

            <hr>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h6>
                        Note: Due to the legal and privacy concerns, <u>we cannot directly share the downloaded videos or clips from YouTube in any way (including but not limited to email, online drives and GitHub).</u> However, there are many open-source tools to download the original clips (e.g., <a href="https://github.com/activitynet/ActivityNet/tree/master/Crawler/Kinetics" target="_blank">[Tool #1]</a> and <a href="https://github.com/ytdl-org/youtube-dl" target="_blank">[Tool #2]</a>). Some videos might be unavailable (deleted or hidden by either YouTube or the users) at this moment, but they were available when we collected the dataset. Considering that it is an extremely small percentage, we expect that it won't have a significant impact on the performance. 
                    </h6>
                </div>

                <br>
                <br>
                <br>

                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h6>In addition to the YouTube video ids, we provide the pretrained video features below for quick development. The features including all the videos are extracted using a pretrained I3D model <a href="https://github.com/eric-xw/kinetics-i3d-pytorch">[here]</a>. Each video is represented by a numpy array of size (1, num_of_segments, 1024). </h6>
                </div>

                <br>
                <br>
                <br>

                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h5>I3D Features on AWS S3:</h5>
                    <ul>
                        <li>Train & Validation Sets (3.0 GB): <a href="https://vatex-feats.s3.amazonaws.com/trainval.zip">https://vatex-feats.s3.amazonaws.com/trainval.zip</a></li>
                        <li>Public Test Set (634.9 MB): <a href="https://vatex-feats.s3.amazonaws.com/public_test.zip">https://vatex-feats.s3.amazonaws.com/public_test.zip</a></li>
                        <li>Private Test Set (665.1 MB): <a href="https://vatex-feats.s3.amazonaws.com/private_test.zip">https://vatex-feats.s3.amazonaws.com/private_test.zip</a></li>
                    </ul>
                </div>
            </div>
        </div>

        <br/>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Annotation Format</h4>
                </div>
            </div>

            <hr>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <pre><code>
{
    'videoID': 'YouTubeID_StartTime_EndTime',
    'enCap': 
        [
            'Regular English Caption #1',
            'Regular English Caption #2',
            'Regular English Caption #3',
            'Regular English Caption #4',
            'Regular English Caption #5',
            'Parallel English Caption #1',
            'Parallel English Caption #2',
            'Parallel English Caption #3',
            'Parallel English Caption #4',
            'Parallel English Caption #5'
        ],
    'chCap': 
        [
            'Regular Chinese Caption #1',
            'Regular Chinese Caption #2',
            'Regular Chinese Caption #3',
            'Regular Chinese Caption #4',
            'Regular Chinese Caption #5',
            'Parallel Chinese Caption #1',
            'Parallel Chinese Caption #2',
            'Parallel Chinese Caption #3',
            'Parallel Chinese Caption #4',
            'Parallel Chinese Caption #5'
        ]
}
                </code></pre>
                </div>
            </div>
        </div> -->

        <br/>

        <hr>

        <div class="container footer">
            <div class="container footer">
                <div class="row">
                    <div class="col-lg-6 col-md-6 col-sm-6 col-xs-12 thumb">
                        <p>
                            The dataset is under a <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution 4.0 International License</a>.<br/>
                            Contact the FETA team by <a href="mailto:feta.benchmark@gmail.com" target="_blank">feta.benchmark@gmail.com</a>.
                        </p>
                    </div>
                </div>
            </div>
        </body>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
</html>
